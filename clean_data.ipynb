{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CHUNK_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def split_data(row):\n",
    "    eid, values = row\n",
    "    input_ids = tokenizer(values.text).input_ids\n",
    "    chunked = [input_ids[chunk: chunk + CHUNK_SIZE] for chunk in range(0, len(input_ids), CHUNK_SIZE)]\n",
    "    decoded_chunked = tokenizer.batch_decode(chunked)\n",
    "    return pd.DataFrame({'id': [eid]*len(chunked),\n",
    "                         'pretokenized_text': chunked,\n",
    "                         'decoded_text': decoded_chunked})\n",
    "                         \n",
    "def build_chunk_dataframe(text_data, metadata=None, cores=10):\n",
    "    with Pool(cores) as p:\n",
    "        chunks = list(tqdm(p.imap_unordered(split_data, text_data.iterrows()),\n",
    "                            total=len(text_data)))\n",
    "    \n",
    "    if metadata is not None:\n",
    "        return pd.concat(chunks).merge(metadata, on='id')\n",
    "    else:\n",
    "        return pd.concat(chunks)\n",
    "\n",
    "def clean_non_unique(data):\n",
    "    nunique_ids = (data.id.value_counts() > 1)\n",
    "    nunique_ids = nunique_ids[nunique_ids].index\n",
    "    return data[data.id.isin(nunique_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load data blog_as_csv.csv')\n",
    "blog_corpus = pd.read_csv(\"data/nlp/blog_corpus/blog_as_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>gender</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blog_0</th>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>Info has been found (+/- 100 pages, and 4.5 MB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blog_1</th>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>male</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can now 'capture'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blog_10</th>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>Even though I am exhausted after today, I must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blog_100</th>\n",
       "      <td>26</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>Hello again.  This is the offical No Action bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blog_1000</th>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>My 'band' got in its first fight tonight. most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blog_9995</th>\n",
       "      <td>17</td>\n",
       "      <td>Communications-Media</td>\n",
       "      <td>male</td>\n",
       "      <td>Good morning folks,  How are me brothers and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blog_9996</th>\n",
       "      <td>23</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>NEWater   Ok, that's just gross.   Another pot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blog_9997</th>\n",
       "      <td>26</td>\n",
       "      <td>Education</td>\n",
       "      <td>male</td>\n",
       "      <td>I love salsa. It's one of the greatest foods e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blog_9998</th>\n",
       "      <td>13</td>\n",
       "      <td>Law</td>\n",
       "      <td>male</td>\n",
       "      <td>Hey all, This is Jared, this is my first post ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blog_9999</th>\n",
       "      <td>33</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>On the underground in London, at rush hour, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19320 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age                 topic  gender  \\\n",
       "id                                             \n",
       "blog_0      15               Student    male   \n",
       "blog_1      33     InvestmentBanking    male   \n",
       "blog_10     25                indUnk  female   \n",
       "blog_100    26                indUnk    male   \n",
       "blog_1000   16               Student    male   \n",
       "...        ...                   ...     ...   \n",
       "blog_9995   17  Communications-Media    male   \n",
       "blog_9996   23                indUnk  female   \n",
       "blog_9997   26             Education    male   \n",
       "blog_9998   13                   Law    male   \n",
       "blog_9999   33                indUnk  female   \n",
       "\n",
       "                                                        text  \n",
       "id                                                            \n",
       "blog_0     Info has been found (+/- 100 pages, and 4.5 MB...  \n",
       "blog_1     Thanks to Yahoo!'s Toolbar I can now 'capture'...  \n",
       "blog_10    Even though I am exhausted after today, I must...  \n",
       "blog_100   Hello again.  This is the offical No Action bl...  \n",
       "blog_1000  My 'band' got in its first fight tonight. most...  \n",
       "...                                                      ...  \n",
       "blog_9995  Good morning folks,  How are me brothers and s...  \n",
       "blog_9996  NEWater   Ok, that's just gross.   Another pot...  \n",
       "blog_9997  I love salsa. It's one of the greatest foods e...  \n",
       "blog_9998  Hey all, This is Jared, this is my first post ...  \n",
       "blog_9999  On the underground in London, at rush hour, th...  \n",
       "\n",
       "[19320 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_corpus.text = blog_corpus.text.apply(lambda x: x.strip())\n",
    "clean_blog_corpus = blog_corpus[['id', 'text']].groupby(\"id\").agg(lambda x: '<\\s>'.join(x))\n",
    "meta_blog_corpus = blog_corpus[['id', 'age', 'topic', 'gender']].groupby(\"id\").agg(lambda x: list(x)[0])\n",
    "full_blog_corpus = meta_blog_corpus.merge(clean_blog_corpus, on='id')\n",
    "full_blog_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7834874141704c4f83aada41dd7e04f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pretokenized_text</th>\n",
       "      <th>decoded_text</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blog_10</td>\n",
       "      <td>[0, 8170, 600, 38, 524, 17067, 71, 452, 6, 38,...</td>\n",
       "      <td>&lt;s&gt;Even though I am exhausted after today, I m...</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blog_10</td>\n",
       "      <td>[216, 24, 4, 152, 7105, 16, 3680, 684, 25, 121...</td>\n",
       "      <td>know it. This hell is otherwise known as U Vi...</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blog_10</td>\n",
       "      <td>[17027, 12, 560, 12, 1610, 18, 2850, 12179, 33...</td>\n",
       "      <td>groom-to-be's scrotal bling, but not the Fout...</td>\n",
       "      <td>25</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blog_100</td>\n",
       "      <td>[0, 31414, 456, 4, 1437, 152, 16, 5, 160, 3569...</td>\n",
       "      <td>&lt;s&gt;Hello again.  This is the offical No Action...</td>\n",
       "      <td>26</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blog_100</td>\n",
       "      <td>[5, 7884, 20774, 29, 31, 14, 6, 25, 157, 25, 5...</td>\n",
       "      <td>the singalongs from that, as well as the mino...</td>\n",
       "      <td>26</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382132</th>\n",
       "      <td>blog_9660</td>\n",
       "      <td>[6, 53, 38, 174, 69, 52, 1017, 1153, 357, 2067...</td>\n",
       "      <td>, but I told her we'd probably better wait on ...</td>\n",
       "      <td>35</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382133</th>\n",
       "      <td>blog_9660</td>\n",
       "      <td>[6, 61, 16, 182, 9327, 4, 1437, 1437, 38, 21, ...</td>\n",
       "      <td>, which is very unfortunate.   I was pretty un...</td>\n",
       "      <td>35</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382134</th>\n",
       "      <td>blog_9660</td>\n",
       "      <td>[9, 5, 2859, 9572, 6, 30005, 24, 6, 8, 122, 52...</td>\n",
       "      <td>of the heat strip, disconnected it, and now w...</td>\n",
       "      <td>35</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382135</th>\n",
       "      <td>blog_9660</td>\n",
       "      <td>[24, 19, 162, 8, 3668, 19975, 24, 31509, 243, ...</td>\n",
       "      <td>it with me and absolutely hated it ('It's stu...</td>\n",
       "      <td>35</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382136</th>\n",
       "      <td>blog_9660</td>\n",
       "      <td>[9366, 6, 11954, 6, 132, 12, 19402, 9, 26108, ...</td>\n",
       "      <td>pizza, wings, 2-liter of Coke, etc... Pretty ...</td>\n",
       "      <td>35</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382024 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                  pretokenized_text  \\\n",
       "0         blog_10  [0, 8170, 600, 38, 524, 17067, 71, 452, 6, 38,...   \n",
       "1         blog_10  [216, 24, 4, 152, 7105, 16, 3680, 684, 25, 121...   \n",
       "2         blog_10  [17027, 12, 560, 12, 1610, 18, 2850, 12179, 33...   \n",
       "3        blog_100  [0, 31414, 456, 4, 1437, 152, 16, 5, 160, 3569...   \n",
       "4        blog_100  [5, 7884, 20774, 29, 31, 14, 6, 25, 157, 25, 5...   \n",
       "...           ...                                                ...   \n",
       "382132  blog_9660  [6, 53, 38, 174, 69, 52, 1017, 1153, 357, 2067...   \n",
       "382133  blog_9660  [6, 61, 16, 182, 9327, 4, 1437, 1437, 38, 21, ...   \n",
       "382134  blog_9660  [9, 5, 2859, 9572, 6, 30005, 24, 6, 8, 122, 52...   \n",
       "382135  blog_9660  [24, 19, 162, 8, 3668, 19975, 24, 31509, 243, ...   \n",
       "382136  blog_9660  [9366, 6, 11954, 6, 132, 12, 19402, 9, 26108, ...   \n",
       "\n",
       "                                             decoded_text  age   topic  gender  \n",
       "0       <s>Even though I am exhausted after today, I m...   25  indUnk  female  \n",
       "1        know it. This hell is otherwise known as U Vi...   25  indUnk  female  \n",
       "2        groom-to-be's scrotal bling, but not the Fout...   25  indUnk  female  \n",
       "3       <s>Hello again.  This is the offical No Action...   26  indUnk    male  \n",
       "4        the singalongs from that, as well as the mino...   26  indUnk    male  \n",
       "...                                                   ...  ...     ...     ...  \n",
       "382132  , but I told her we'd probably better wait on ...   35  indUnk    male  \n",
       "382133  , which is very unfortunate.   I was pretty un...   35  indUnk    male  \n",
       "382134   of the heat strip, disconnected it, and now w...   35  indUnk    male  \n",
       "382135   it with me and absolutely hated it ('It's stu...   35  indUnk    male  \n",
       "382136   pizza, wings, 2-liter of Coke, etc... Pretty ...   35  indUnk    male  \n",
       "\n",
       "[382024 rows x 6 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_blog_data = build_chunk_dataframe(full_blog_corpus, meta_blog_corpus)\n",
    "nunique_blog_data = clean_non_unique(chunked_blog_data)\n",
    "nunique_blog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_blog_data.to_csv(\"data/nlp/blog_corpus/blog_as_csv_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mail data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load data mail_as_csv.csv')\n",
    "mail_corpus = pd.read_csv(\"data/nlp/enron_mail_20150507/mail_as_csv.csv\")\n",
    "mail_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    clean_mail = re.sub(r'(\\\\+r)?(\\\\+n)+', '\\n', text)\n",
    "    clean_mail = re.sub(r'\\\\+t', '\\t', clean_mail)\n",
    "    clean_mail = '\\n'.join(clean_mail.strip().split('\\n')[15:-1])\n",
    "    clean_mail = re.sub(r'X-.+:.*\\n', '<s>', clean_mail)\n",
    "    clean_mail = re.sub(r'From:.*\\n', '', clean_mail)\n",
    "    clean_mail = re.sub(r\"\\\\'\", \"'\", clean_mail)\n",
    "\n",
    "    return clean_mail\n",
    "\n",
    "mail_corpus['clean_text'] = mail_corpus.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e96f30748cf4e2288502fc92cb341f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (460644 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (454546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (505057 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691872 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (863829 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (809679 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1077318 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2207991 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2625737 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2445533 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pretokenized_text</th>\n",
       "      <th>decoded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mail_101</td>\n",
       "      <td>[0, 28915, 6, 50118, 6715, 3438, 162, 31, 110,...</td>\n",
       "      <td>&lt;s&gt;Brian,\\nPlease remove me from your distribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mail_101</td>\n",
       "      <td>[4, 1437, 22381, 42, 16, 230, 35, 48669, 44426...</td>\n",
       "      <td>.  Usually this is C:\\\\Program Files\\\\Microsof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mail_101</td>\n",
       "      <td>[50118, 39767, 21194, 41552, 37457, 29, 15698,...</td>\n",
       "      <td>\\nGabriel&lt;\\s&gt;Hey do you have to go to the harr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mail_101</td>\n",
       "      <td>[87, 706, 722, 49069, 37457, 29, 15698, 100, 2...</td>\n",
       "      <td>than 24 hours.&lt;\\s&gt;I think harassing me is a v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mail_101</td>\n",
       "      <td>[31271, 4, 50118, 176, 73, 1549, 73, 2663, 143...</td>\n",
       "      <td>520.\\n2/16/01  MANAGEMENT-PWR 177,196.&lt;\\s&gt;I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63596</th>\n",
       "      <td>mail_19</td>\n",
       "      <td>[246, 495, 5214, 246, 495, 5214, 246, 495, 521...</td>\n",
       "      <td>3D=3D=3D=3D=3D\\nThe object of humor notwithsta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63597</th>\n",
       "      <td>mail_19</td>\n",
       "      <td>[4, 1437, 6830, 5, 414, 6, 79, 64, 75, 224, 93...</td>\n",
       "      <td>.  Without the data, she can't say anything\\nc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63598</th>\n",
       "      <td>mail_19</td>\n",
       "      <td>[50118, 42038, 1258, 4, 1437, 38, 74, 28, 55, ...</td>\n",
       "      <td>\\nparticipation.  I would be more than happy t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63599</th>\n",
       "      <td>mail_19</td>\n",
       "      <td>[0, 0, 0, 0, 0, 50118, 28409, 100, 4, 1437, 45...</td>\n",
       "      <td>&lt;s&gt;&lt;s&gt;&lt;s&gt;&lt;s&gt;&lt;s&gt;\\nFYI.  Thanks to Max at the PX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63600</th>\n",
       "      <td>mail_19</td>\n",
       "      <td>[3170, 4, 612, 1039, 44460, 11762, 38740, 4, 1...</td>\n",
       "      <td>38.00@*.calpx.com&gt;\\nDate: Tue, 22 Aug 2000 11:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>613635 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                  pretokenized_text  \\\n",
       "0      mail_101  [0, 28915, 6, 50118, 6715, 3438, 162, 31, 110,...   \n",
       "1      mail_101  [4, 1437, 22381, 42, 16, 230, 35, 48669, 44426...   \n",
       "2      mail_101  [50118, 39767, 21194, 41552, 37457, 29, 15698,...   \n",
       "3      mail_101  [87, 706, 722, 49069, 37457, 29, 15698, 100, 2...   \n",
       "4      mail_101  [31271, 4, 50118, 176, 73, 1549, 73, 2663, 143...   \n",
       "...         ...                                                ...   \n",
       "63596   mail_19  [246, 495, 5214, 246, 495, 5214, 246, 495, 521...   \n",
       "63597   mail_19  [4, 1437, 6830, 5, 414, 6, 79, 64, 75, 224, 93...   \n",
       "63598   mail_19  [50118, 42038, 1258, 4, 1437, 38, 74, 28, 55, ...   \n",
       "63599   mail_19  [0, 0, 0, 0, 0, 50118, 28409, 100, 4, 1437, 45...   \n",
       "63600   mail_19  [3170, 4, 612, 1039, 44460, 11762, 38740, 4, 1...   \n",
       "\n",
       "                                            decoded_text  \n",
       "0      <s>Brian,\\nPlease remove me from your distribu...  \n",
       "1      .  Usually this is C:\\\\Program Files\\\\Microsof...  \n",
       "2      \\nGabriel<\\s>Hey do you have to go to the harr...  \n",
       "3       than 24 hours.<\\s>I think harassing me is a v...  \n",
       "4      520.\\n2/16/01  MANAGEMENT-PWR 177,196.<\\s>I'm ...  \n",
       "...                                                  ...  \n",
       "63596  3D=3D=3D=3D=3D\\nThe object of humor notwithsta...  \n",
       "63597  .  Without the data, she can't say anything\\nc...  \n",
       "63598  \\nparticipation.  I would be more than happy t...  \n",
       "63599  <s><s><s><s><s>\\nFYI.  Thanks to Max at the PX...  \n",
       "63600  38.00@*.calpx.com>\\nDate: Tue, 22 Aug 2000 11:...  \n",
       "\n",
       "[613635 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mail_corpus.columns = ['user', 'old_text', 'id', 'text']\n",
    "mail_corpus.text = mail_corpus.text.apply(lambda x: x.strip())\n",
    "clean_mail_corpus = mail_corpus[['id', 'text']].groupby(\"id\").agg(lambda x: '<\\s>'.join(x))\n",
    "\n",
    "chunked_mail_data = build_chunk_dataframe(clean_mail_corpus, None)\n",
    "nunique_mail_data = clean_non_unique(chunked_mail_data)\n",
    "nunique_mail_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_mail_data.to_csv(\"data/nlp/enron_mail_20150507/mail_as_csv_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data book_as_csv.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/nlp/gutenberg/book_as_csv.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_490029/3546177140.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load data book_as_csv.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbook_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/nlp/gutenberg/book_as_csv.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbook_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/nlp/gutenberg/book_as_csv.csv'"
     ]
    }
   ],
   "source": [
    "print('Load data book_as_csv.csv')\n",
    "book_corpus = pd.read_csv(\"data/nlp/gutenberg/book_as_csv.csv\")\n",
    "book_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\n\\n+', '\\n', text)[512:]\n",
    "\n",
    "book_corpus['clean_text'] = book_corpus.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da1d4712d374e88ba23b1446bd224cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (39958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (66207 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (63482 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (65489 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (61438 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (113469 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (128749 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (195722 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (284486 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1780828 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pretokenized_text</th>\n",
       "      <th>decoded_text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>id_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PG10007</td>\n",
       "      <td>[0, 39986, 10, 2225, 7391, 7, 5, 36455, 3693, ...</td>\n",
       "      <td>&lt;s&gt;Upon a paper attached to the Narrative whic...</td>\n",
       "      <td>Carmilla</td>\n",
       "      <td>Le Fanu, Joseph Sheridan</td>\n",
       "      <td>1814.0</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>3626</td>\n",
       "      <td>{'Vampires -- Fiction', 'Young women -- Fiction'}</td>\n",
       "      <td>book_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PG10007</td>\n",
       "      <td>[15, 10, 7019, 50118, 20554, 4086, 11, 10, 669...</td>\n",
       "      <td>on a slight\\neminence in a forest. The road, ...</td>\n",
       "      <td>Carmilla</td>\n",
       "      <td>Le Fanu, Joseph Sheridan</td>\n",
       "      <td>1814.0</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>3626</td>\n",
       "      <td>{'Vampires -- Fiction', 'Young women -- Fiction'}</td>\n",
       "      <td>book_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PG10007</td>\n",
       "      <td>[6, 50118, 8155, 56, 57, 19, 162, 31, 6, 38, 4...</td>\n",
       "      <td>,\\nwho had been with me from, I might almost s...</td>\n",
       "      <td>Carmilla</td>\n",
       "      <td>Le Fanu, Joseph Sheridan</td>\n",
       "      <td>1814.0</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>3626</td>\n",
       "      <td>{'Vampires -- Fiction', 'Young women -- Fiction'}</td>\n",
       "      <td>book_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PG10007</td>\n",
       "      <td>[802, 2185, 1937, 4, 38, 21, 45, 26851, 6, 13,...</td>\n",
       "      <td>thought myself alone. I was not frightened, f...</td>\n",
       "      <td>Carmilla</td>\n",
       "      <td>Le Fanu, Joseph Sheridan</td>\n",
       "      <td>1814.0</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>3626</td>\n",
       "      <td>{'Vampires -- Fiction', 'Young women -- Fiction'}</td>\n",
       "      <td>book_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PG10007</td>\n",
       "      <td>[70, 363, 131, 8, 31, 14, 86, 10, 20667, 50118...</td>\n",
       "      <td>all night; and from that time a servant\\nalwa...</td>\n",
       "      <td>Carmilla</td>\n",
       "      <td>Le Fanu, Joseph Sheridan</td>\n",
       "      <td>1814.0</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>3626</td>\n",
       "      <td>{'Vampires -- Fiction', 'Young women -- Fiction'}</td>\n",
       "      <td>book_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689426</th>\n",
       "      <td>PG36734</td>\n",
       "      <td>[154, 36, 13728, 4, 939, 6, 33906, 4, 28222, 1...</td>\n",
       "      <td>ing (vol. i, pp. 169-70, of the\\n    seventeen...</td>\n",
       "      <td>The Browning CyclopÃ¦dia: A Guide to the Study ...</td>\n",
       "      <td>Berdoe, Edward</td>\n",
       "      <td>1836.0</td>\n",
       "      <td>1916.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>100</td>\n",
       "      <td>{'Browning, Robert, 1812-1889 -- Encyclopedias'}</td>\n",
       "      <td>book_1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689427</th>\n",
       "      <td>PG36734</td>\n",
       "      <td>[28173, 4, 925, 4, 24030, 2645, 5789, 7, 162, ...</td>\n",
       "      <td>satisfactory. Dr. Garnett writes to me on the...</td>\n",
       "      <td>The Browning CyclopÃ¦dia: A Guide to the Study ...</td>\n",
       "      <td>Berdoe, Edward</td>\n",
       "      <td>1836.0</td>\n",
       "      <td>1916.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>100</td>\n",
       "      <td>{'Browning, Robert, 1812-1889 -- Encyclopedias'}</td>\n",
       "      <td>book_1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689428</th>\n",
       "      <td>PG36734</td>\n",
       "      <td>[221, 2028, 271, 18, 22, 46354, 46439, 811, 38...</td>\n",
       "      <td>Pindar's \"Fourth Pythian Ode,\" where he speak...</td>\n",
       "      <td>The Browning CyclopÃ¦dia: A Guide to the Study ...</td>\n",
       "      <td>Berdoe, Edward</td>\n",
       "      <td>1836.0</td>\n",
       "      <td>1916.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>100</td>\n",
       "      <td>{'Browning, Robert, 1812-1889 -- Encyclopedias'}</td>\n",
       "      <td>book_1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689429</th>\n",
       "      <td>PG36734</td>\n",
       "      <td>[11005, 4, 50118, 10975, 401, 742, 20, 1065, 9...</td>\n",
       "      <td>Bible.\\n[6] The above sonnet, by Robert Brown...</td>\n",
       "      <td>The Browning CyclopÃ¦dia: A Guide to the Study ...</td>\n",
       "      <td>Berdoe, Edward</td>\n",
       "      <td>1836.0</td>\n",
       "      <td>1916.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>100</td>\n",
       "      <td>{'Browning, Robert, 1812-1889 -- Encyclopedias'}</td>\n",
       "      <td>book_1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689430</th>\n",
       "      <td>PG36734</td>\n",
       "      <td>[113, 36, 8596, 23703, 43, 50118, 1437, 22, 10...</td>\n",
       "      <td>\" (page 142)\\n  \"seeks\" corrected to \"seek\" (p...</td>\n",
       "      <td>The Browning CyclopÃ¦dia: A Guide to the Study ...</td>\n",
       "      <td>Berdoe, Edward</td>\n",
       "      <td>1836.0</td>\n",
       "      <td>1916.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>100</td>\n",
       "      <td>{'Browning, Robert, 1812-1889 -- Encyclopedias'}</td>\n",
       "      <td>book_1281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>689430 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                  pretokenized_text  \\\n",
       "0       PG10007  [0, 39986, 10, 2225, 7391, 7, 5, 36455, 3693, ...   \n",
       "1       PG10007  [15, 10, 7019, 50118, 20554, 4086, 11, 10, 669...   \n",
       "2       PG10007  [6, 50118, 8155, 56, 57, 19, 162, 31, 6, 38, 4...   \n",
       "3       PG10007  [802, 2185, 1937, 4, 38, 21, 45, 26851, 6, 13,...   \n",
       "4       PG10007  [70, 363, 131, 8, 31, 14, 86, 10, 20667, 50118...   \n",
       "...         ...                                                ...   \n",
       "689426  PG36734  [154, 36, 13728, 4, 939, 6, 33906, 4, 28222, 1...   \n",
       "689427  PG36734  [28173, 4, 925, 4, 24030, 2645, 5789, 7, 162, ...   \n",
       "689428  PG36734  [221, 2028, 271, 18, 22, 46354, 46439, 811, 38...   \n",
       "689429  PG36734  [11005, 4, 50118, 10975, 401, 742, 20, 1065, 9...   \n",
       "689430  PG36734  [113, 36, 8596, 23703, 43, 50118, 1437, 22, 10...   \n",
       "\n",
       "                                             decoded_text  \\\n",
       "0       <s>Upon a paper attached to the Narrative whic...   \n",
       "1        on a slight\\neminence in a forest. The road, ...   \n",
       "2       ,\\nwho had been with me from, I might almost s...   \n",
       "3        thought myself alone. I was not frightened, f...   \n",
       "4        all night; and from that time a servant\\nalwa...   \n",
       "...                                                   ...   \n",
       "689426  ing (vol. i, pp. 169-70, of the\\n    seventeen...   \n",
       "689427   satisfactory. Dr. Garnett writes to me on the...   \n",
       "689428   Pindar's \"Fourth Pythian Ode,\" where he speak...   \n",
       "689429   Bible.\\n[6] The above sonnet, by Robert Brown...   \n",
       "689430  \" (page 142)\\n  \"seeks\" corrected to \"seek\" (p...   \n",
       "\n",
       "                                                    title  \\\n",
       "0                                                Carmilla   \n",
       "1                                                Carmilla   \n",
       "2                                                Carmilla   \n",
       "3                                                Carmilla   \n",
       "4                                                Carmilla   \n",
       "...                                                   ...   \n",
       "689426  The Browning CyclopÃ¦dia: A Guide to the Study ...   \n",
       "689427  The Browning CyclopÃ¦dia: A Guide to the Study ...   \n",
       "689428  The Browning CyclopÃ¦dia: A Guide to the Study ...   \n",
       "689429  The Browning CyclopÃ¦dia: A Guide to the Study ...   \n",
       "689430  The Browning CyclopÃ¦dia: A Guide to the Study ...   \n",
       "\n",
       "                          author  authoryearofbirth  authoryearofdeath  \\\n",
       "0       Le Fanu, Joseph Sheridan             1814.0             1873.0   \n",
       "1       Le Fanu, Joseph Sheridan             1814.0             1873.0   \n",
       "2       Le Fanu, Joseph Sheridan             1814.0             1873.0   \n",
       "3       Le Fanu, Joseph Sheridan             1814.0             1873.0   \n",
       "4       Le Fanu, Joseph Sheridan             1814.0             1873.0   \n",
       "...                          ...                ...                ...   \n",
       "689426            Berdoe, Edward             1836.0             1916.0   \n",
       "689427            Berdoe, Edward             1836.0             1916.0   \n",
       "689428            Berdoe, Edward             1836.0             1916.0   \n",
       "689429            Berdoe, Edward             1836.0             1916.0   \n",
       "689430            Berdoe, Edward             1836.0             1916.0   \n",
       "\n",
       "       language  downloads                                           subjects  \\\n",
       "0        ['en']       3626  {'Vampires -- Fiction', 'Young women -- Fiction'}   \n",
       "1        ['en']       3626  {'Vampires -- Fiction', 'Young women -- Fiction'}   \n",
       "2        ['en']       3626  {'Vampires -- Fiction', 'Young women -- Fiction'}   \n",
       "3        ['en']       3626  {'Vampires -- Fiction', 'Young women -- Fiction'}   \n",
       "4        ['en']       3626  {'Vampires -- Fiction', 'Young women -- Fiction'}   \n",
       "...         ...        ...                                                ...   \n",
       "689426   ['en']        100   {'Browning, Robert, 1812-1889 -- Encyclopedias'}   \n",
       "689427   ['en']        100   {'Browning, Robert, 1812-1889 -- Encyclopedias'}   \n",
       "689428   ['en']        100   {'Browning, Robert, 1812-1889 -- Encyclopedias'}   \n",
       "689429   ['en']        100   {'Browning, Robert, 1812-1889 -- Encyclopedias'}   \n",
       "689430   ['en']        100   {'Browning, Robert, 1812-1889 -- Encyclopedias'}   \n",
       "\n",
       "             id_2  \n",
       "0          book_0  \n",
       "1          book_0  \n",
       "2          book_0  \n",
       "3          book_0  \n",
       "4          book_0  \n",
       "...           ...  \n",
       "689426  book_1281  \n",
       "689427  book_1281  \n",
       "689428  book_1281  \n",
       "689429  book_1281  \n",
       "689430  book_1281  \n",
       "\n",
       "[689430 rows x 11 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_corpus.columns = ['old_text', 'id', 'title', 'author', 'authoryearofbirth',\n",
    "                        'authoryearofdeath', 'language', 'downloads', 'subjects', 'id_2',\n",
    "                        'text']\n",
    "book_corpus.text = book_corpus.text.apply(lambda x: x.strip())\n",
    "clean_book_corpus = book_corpus[['id', 'text']].groupby(\"id\").agg(lambda x: '<\\s>'.join(x))\n",
    "\n",
    "chunked_book_data = build_chunk_dataframe(clean_book_corpus, book_corpus.drop(['old_text', 'text'], axis=1))\n",
    "nunique_book_data = clean_non_unique(chunked_book_data)\n",
    "nunique_book_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'id', 'title', 'author', 'authoryearofbirth',\n",
       "       'authoryearofdeath', 'language', 'downloads', 'subjects', 'id_2',\n",
       "       'clean_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_corpus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_book_data.to_csv(\"data/nlp/gutenberg/book_as_csv_preprocessed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
